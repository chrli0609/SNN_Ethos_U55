Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 4, 4, 16)          1040      
                                                                 
=================================================================
Total params: 1040 (4.06 KB)
Trainable params: 1040 (4.06 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
[<tf.Variable 'conv2d/kernel:0' shape=(2, 2, 16, 16) dtype=float32, numpy=
array([[[[-0.13369846,  0.1689115 ,  0.11100246, ..., -0.05142918,
          -0.01886876, -0.21524483],
         [-0.18405582,  0.17176051, -0.12943782, ...,  0.18392466,
           0.02903979,  0.03936367],
         [ 0.20743878, -0.15502316, -0.05335091, ...,  0.00148797,
          -0.03314465,  0.20470037],
         ...,
         [-0.20523015, -0.20792879,  0.07607917, ...,  0.18050493,
          -0.14142384,  0.04867767],
         [-0.06942996, -0.1613992 ,  0.07515602, ...,  0.15762286,
          -0.21162818,  0.10407461],
         [-0.21340099,  0.06074549, -0.10748503, ...,  0.04280259,
           0.10818292, -0.00859846]],

        [[ 0.03989111, -0.09462459, -0.00923209, ..., -0.14160621,
           0.00368911, -0.15234546],
         [-0.07121877, -0.0932603 , -0.18462937, ..., -0.20750697,
          -0.18484765, -0.05764283],
         [ 0.16099001, -0.20924632,  0.14002259, ...,  0.1788782 ,
           0.02121887, -0.16689515],
         ...,
         [-0.16538653, -0.07271867,  0.05414002, ..., -0.07861941,
           0.20752813,  0.03831528],
         [ 0.13690881, -0.13566598,  0.04394059, ...,  0.06331272,
           0.02834205,  0.17709167],
         [ 0.19036885, -0.03310217, -0.16813499, ..., -0.00869112,
          -0.11460011,  0.07138948]]],


       [[[-0.2043216 , -0.05550492,  0.07973777, ..., -0.05244529,
           0.00984789,  0.10919379],
         [ 0.12962897,  0.03748222,  0.01992597, ...,  0.09153901,
          -0.07308826,  0.00436229],
         [-0.10992878, -0.13766566,  0.20717205, ...,  0.08232205,
          -0.06534936,  0.08270465],
         ...,
         [ 0.2142175 , -0.11032739, -0.18494397, ...,  0.11887361,
          -0.20441735, -0.08296975],
         [ 0.03201687, -0.09173954, -0.02602312, ..., -0.19452088,
          -0.11087656,  0.01109853],
         [-0.13038741,  0.08166824, -0.05904047, ..., -0.20231883,
           0.14215218, -0.1768449 ]],

        [[-0.21142806, -0.16150993,  0.06660362, ...,  0.08689164,
           0.15148039, -0.02349047],
         [ 0.14655022, -0.02088086, -0.09761034, ...,  0.01395151,
           0.07993348, -0.03544614],
         [-0.135547  ,  0.14124529, -0.12384819, ...,  0.07133351,
          -0.00535265,  0.0196418 ],
         ...,
         [ 0.1750422 ,  0.15894611, -0.01004168, ..., -0.11844382,
          -0.08281898,  0.11120103],
         [ 0.16009323,  0.09050311, -0.14981464, ..., -0.20393306,
          -0.14171475, -0.10361607],
         [-0.20270917, -0.12035378,  0.12180753, ..., -0.09718154,
           0.0322154 , -0.00580446]]]], dtype=float32)>, <tf.Variable 'conv2d/bias:0' shape=(16,) dtype=float32, numpy=
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
      dtype=float32)>]
model layers:
 [<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f341963ff10>]
model weights:
weight 0 : (2, 2, 16, 16) 
 [[[[-0.13369846  0.1689115   0.11100246 ... -0.05142918 -0.01886876
    -0.21524483]
   [-0.18405582  0.17176051 -0.12943782 ...  0.18392466  0.02903979
     0.03936367]
   [ 0.20743878 -0.15502316 -0.05335091 ...  0.00148797 -0.03314465
     0.20470037]
   ...
   [-0.20523015 -0.20792879  0.07607917 ...  0.18050493 -0.14142384
     0.04867767]
   [-0.06942996 -0.1613992   0.07515602 ...  0.15762286 -0.21162818
     0.10407461]
   [-0.21340099  0.06074549 -0.10748503 ...  0.04280259  0.10818292
    -0.00859846]]

  [[ 0.03989111 -0.09462459 -0.00923209 ... -0.14160621  0.00368911
    -0.15234546]
   [-0.07121877 -0.0932603  -0.18462937 ... -0.20750697 -0.18484765
    -0.05764283]
   [ 0.16099001 -0.20924632  0.14002259 ...  0.1788782   0.02121887
    -0.16689515]
   ...
   [-0.16538653 -0.07271867  0.05414002 ... -0.07861941  0.20752813
     0.03831528]
   [ 0.13690881 -0.13566598  0.04394059 ...  0.06331272  0.02834205
     0.17709167]
   [ 0.19036885 -0.03310217 -0.16813499 ... -0.00869112 -0.11460011
     0.07138948]]]


 [[[-0.2043216  -0.05550492  0.07973777 ... -0.05244529  0.00984789
     0.10919379]
   [ 0.12962897  0.03748222  0.01992597 ...  0.09153901 -0.07308826
     0.00436229]
   [-0.10992878 -0.13766566  0.20717205 ...  0.08232205 -0.06534936
     0.08270465]
   ...
   [ 0.2142175  -0.11032739 -0.18494397 ...  0.11887361 -0.20441735
    -0.08296975]
   [ 0.03201687 -0.09173954 -0.02602312 ... -0.19452088 -0.11087656
     0.01109853]
   [-0.13038741  0.08166824 -0.05904047 ... -0.20231883  0.14215218
    -0.1768449 ]]

  [[-0.21142806 -0.16150993  0.06660362 ...  0.08689164  0.15148039
    -0.02349047]
   [ 0.14655022 -0.02088086 -0.09761034 ...  0.01395151  0.07993348
    -0.03544614]
   [-0.135547    0.14124529 -0.12384819 ...  0.07133351 -0.00535265
     0.0196418 ]
   ...
   [ 0.1750422   0.15894611 -0.01004168 ... -0.11844382 -0.08281898
     0.11120103]
   [ 0.16009323  0.09050311 -0.14981464 ... -0.20393306 -0.14171475
    -0.10361607]
   [-0.20270917 -0.12035378  0.12180753 ... -0.09718154  0.0322154
    -0.00580446]]]]
weight 1 : (16,) 
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
model weights:
 [array([[[[-0.13369846,  0.1689115 ,  0.11100246, ..., -0.05142918,
          -0.01886876, -0.21524483],
         [-0.18405582,  0.17176051, -0.12943782, ...,  0.18392466,
           0.02903979,  0.03936367],
         [ 0.20743878, -0.15502316, -0.05335091, ...,  0.00148797,
          -0.03314465,  0.20470037],
         ...,
         [-0.20523015, -0.20792879,  0.07607917, ...,  0.18050493,
          -0.14142384,  0.04867767],
         [-0.06942996, -0.1613992 ,  0.07515602, ...,  0.15762286,
          -0.21162818,  0.10407461],
         [-0.21340099,  0.06074549, -0.10748503, ...,  0.04280259,
           0.10818292, -0.00859846]],

        [[ 0.03989111, -0.09462459, -0.00923209, ..., -0.14160621,
           0.00368911, -0.15234546],
         [-0.07121877, -0.0932603 , -0.18462937, ..., -0.20750697,
          -0.18484765, -0.05764283],
         [ 0.16099001, -0.20924632,  0.14002259, ...,  0.1788782 ,
           0.02121887, -0.16689515],
         ...,
         [-0.16538653, -0.07271867,  0.05414002, ..., -0.07861941,
           0.20752813,  0.03831528],
         [ 0.13690881, -0.13566598,  0.04394059, ...,  0.06331272,
           0.02834205,  0.17709167],
         [ 0.19036885, -0.03310217, -0.16813499, ..., -0.00869112,
          -0.11460011,  0.07138948]]],


       [[[-0.2043216 , -0.05550492,  0.07973777, ..., -0.05244529,
           0.00984789,  0.10919379],
         [ 0.12962897,  0.03748222,  0.01992597, ...,  0.09153901,
          -0.07308826,  0.00436229],
         [-0.10992878, -0.13766566,  0.20717205, ...,  0.08232205,
          -0.06534936,  0.08270465],
         ...,
         [ 0.2142175 , -0.11032739, -0.18494397, ...,  0.11887361,
          -0.20441735, -0.08296975],
         [ 0.03201687, -0.09173954, -0.02602312, ..., -0.19452088,
          -0.11087656,  0.01109853],
         [-0.13038741,  0.08166824, -0.05904047, ..., -0.20231883,
           0.14215218, -0.1768449 ]],

        [[-0.21142806, -0.16150993,  0.06660362, ...,  0.08689164,
           0.15148039, -0.02349047],
         [ 0.14655022, -0.02088086, -0.09761034, ...,  0.01395151,
           0.07993348, -0.03544614],
         [-0.135547  ,  0.14124529, -0.12384819, ...,  0.07133351,
          -0.00535265,  0.0196418 ],
         ...,
         [ 0.1750422 ,  0.15894611, -0.01004168, ..., -0.11844382,
          -0.08281898,  0.11120103],
         [ 0.16009323,  0.09050311, -0.14981464, ..., -0.20393306,
          -0.14171475, -0.10361607],
         [-0.20270917, -0.12035378,  0.12180753, ..., -0.09718154,
           0.0322154 , -0.00580446]]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
      dtype=float32)]
set weights
model weights:
 [array([[[[0.000e+00, 1.000e+00, 2.000e+00, ..., 1.300e+01, 1.400e+01,
          1.500e+01],
         [1.600e+01, 1.700e+01, 1.800e+01, ..., 2.900e+01, 3.000e+01,
          3.100e+01],
         [3.200e+01, 3.300e+01, 3.400e+01, ..., 4.500e+01, 4.600e+01,
          4.700e+01],
         ...,
         [2.080e+02, 2.090e+02, 2.100e+02, ..., 2.210e+02, 2.220e+02,
          2.230e+02],
         [2.240e+02, 2.250e+02, 2.260e+02, ..., 2.370e+02, 2.380e+02,
          2.390e+02],
         [2.400e+02, 2.410e+02, 2.420e+02, ..., 2.530e+02, 2.540e+02,
          2.550e+02]],

        [[2.560e+02, 2.570e+02, 2.580e+02, ..., 2.690e+02, 2.700e+02,
          2.710e+02],
         [2.720e+02, 2.730e+02, 2.740e+02, ..., 2.850e+02, 2.860e+02,
          2.870e+02],
         [2.880e+02, 2.890e+02, 2.900e+02, ..., 3.010e+02, 3.020e+02,
          3.030e+02],
         ...,
         [4.640e+02, 4.650e+02, 4.660e+02, ..., 4.770e+02, 4.780e+02,
          4.790e+02],
         [4.800e+02, 4.810e+02, 4.820e+02, ..., 4.930e+02, 4.940e+02,
          4.950e+02],
         [4.960e+02, 4.970e+02, 4.980e+02, ..., 5.090e+02, 5.100e+02,
          5.110e+02]]],


       [[[5.120e+02, 5.130e+02, 5.140e+02, ..., 5.250e+02, 5.260e+02,
          5.270e+02],
         [5.280e+02, 5.290e+02, 5.300e+02, ..., 5.410e+02, 5.420e+02,
          5.430e+02],
         [5.440e+02, 5.450e+02, 5.460e+02, ..., 5.570e+02, 5.580e+02,
          5.590e+02],
         ...,
         [7.200e+02, 7.210e+02, 7.220e+02, ..., 7.330e+02, 7.340e+02,
          7.350e+02],
         [7.360e+02, 7.370e+02, 7.380e+02, ..., 7.490e+02, 7.500e+02,
          7.510e+02],
         [7.520e+02, 7.530e+02, 7.540e+02, ..., 7.650e+02, 7.660e+02,
          7.670e+02]],

        [[7.680e+02, 7.690e+02, 7.700e+02, ..., 7.810e+02, 7.820e+02,
          7.830e+02],
         [7.840e+02, 7.850e+02, 7.860e+02, ..., 7.970e+02, 7.980e+02,
          7.990e+02],
         [8.000e+02, 8.010e+02, 8.020e+02, ..., 8.130e+02, 8.140e+02,
          8.150e+02],
         ...,
         [9.760e+02, 9.770e+02, 9.780e+02, ..., 9.890e+02, 9.900e+02,
          9.910e+02],
         [9.920e+02, 9.930e+02, 9.940e+02, ..., 1.005e+03, 1.006e+03,
          1.007e+03],
         [1.008e+03, 1.009e+03, 1.010e+03, ..., 1.021e+03, 1.022e+03,
          1.023e+03]]]], dtype=float32), array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,
       14., 15., 16.], dtype=float32)]
Input shape: (1, 8, 8, 16) 
 tf.Tensor(
[[[[1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   ...
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]]

  [[1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   ...
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]]

  [[1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   ...
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]]

  ...

  [[1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   ...
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]]

  [[1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   ...
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]]

  [[1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   ...
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]
   [1 1 1 ... 1 1 1]]]], shape=(1, 8, 8, 16), dtype=int64)
Output shape: (1, 4, 4, 16) 
 tf.Tensor(
[[[[32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]]

  [[32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]]

  [[32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]]

  [[32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]
   [32257. 32322. 32387. 32452. 32517. 32582. 32647. 32712. 32777.
    32842. 32907. 32972. 33037. 33102. 33167. 33232.]]]], shape=(1, 4, 4, 16), dtype=float32)
signatures _SignatureMap({'serving_default': <ConcreteFunction (*, conv2d_input: TensorSpec(shape=(None, 8, 8, 16), dtype=tf.float32, name='conv2d_input')) -> Dict[['conv2d', TensorSpec(shape=(None, 4, 4, 16), dtype=tf.float32, name='conv2d')]] at 0x7F803B6F7010>})
Quantized TFLite model saved at: saved_models/tflite_model/tflite_model.tflite
Warning: No configuration file specified. Using a default of ['/home/chris/.local/lib/python3.10/site-packages/ethosu/config_files/Arm/vela.ini']. Compilation may be invalid or non-optimal.
Warning: No system configuration specified. Using a default of Ethos_U55_High_End_Embedded. Compilation may be invalid or non-optimal.
Warning: No memory mode specified. Using a default of Shared_Sram. Compilation may be invalid or non-optimal.
Configuration files:
   original = None
   used = ['/home/chris/.local/lib/python3.10/site-packages/ethosu/config_files/Arm/vela.ini']
System Configuration (Ethos_U55_High_End_Embedded):
   core_clock = 500000000.0
   axi0_port = Sram
   axi1_port = OffChipFlash
   Sram_clock_scales = 1.0
   Sram_burst_length = 32
   Sram_read_latency = 32
   Sram_write_latency = 32
   Dram_clock_scales = 1.0
   Dram_burst_length = 1
   Dram_read_latency = 0
   Dram_write_latency = 0
   OnChipFlash_clock_scales = 1.0
   OnChipFlash_burst_length = 1
   OnChipFlash_read_latency = 0
   OnChipFlash_write_latency = 0
   OffChipFlash_clock_scales = 0.125
   OffChipFlash_burst_length = 128
   OffChipFlash_read_latency = 64
   OffChipFlash_write_latency = 0
Memory Mode (Shared_Sram):
   const_mem_area = Axi1
   arena_mem_area = Axi0
   cache_mem_area = Axi0
   arena_cache_size = 4294967296 from Default
Architecture Settings:
   permanent_storage_mem_area = OffChipFlash
   feature_map_storage_mem_area = Sram
   fast_storage_mem_area = Sram
Operators of Subgraph main
  0: Quantize   - {'attribute_read_error': []} - tfl.quantize
  1: Conv2DBias - {'attribute_read_error': [], 'dilation_h_factor': 1, 'dilation_w_factor': 1, 'fused_activation_function': None, 'padding': <Padding.VALID: 1>, 'stride_h': 2, 'stride_w': 2, 'quantized_bias_type': 0, 'strides': (1, 2, 2, 1), 'dilation': (1, 1, 1, 1), 'num_conv_groups': 1} - StatefulPartitionedCall:01
  2: Quantize   - {'attribute_read_error': []} - StatefulPartitionedCall:0
print_graph_with_tensors() main
0 Placeholder serving_default_conv2d_input:0
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_conv2d_input:0' shape=[1, 8, 8, 16] dtype=uint8>

1 AvgPool tfl.quantize
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_conv2d_input:0' shape=[1, 8, 8, 16] dtype=uint8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize' shape=[1, 8, 8, 16] dtype=int8>

2 Const sequential/conv2d/Conv2D_reshape
    Output 00              Weights         OffChipFlash        Permanent_NPU <nng.Tensor 'sequential/conv2d/Conv2D_reshape' shape=[2, 2, 16, 16] dtype=int8>

3 Const sequential/conv2d/BiasAdd/ReadVariableOp_reshape
    Output 00           FeatureMap         OffChipFlash        Permanent_NPU <nng.Tensor 'sequential/conv2d/BiasAdd/ReadVariableOp_reshape' shape=[16] dtype=int32>

4 Conv2DBias StatefulPartitionedCall:01
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize' shape=[1, 8, 8, 16] dtype=int8>
    Input  01              Weights         OffChipFlash        Permanent_NPU <nng.Tensor 'sequential/conv2d/Conv2D_reshape' shape=[2, 2, 16, 16] dtype=int8>
    Input  02           FeatureMap         OffChipFlash        Permanent_NPU <nng.Tensor 'sequential/conv2d/BiasAdd/ReadVariableOp_reshape' shape=[16] dtype=int32>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'StatefulPartitionedCall:01' shape=[1, 4, 4, 16] dtype=int8>

5 AvgPool StatefulPartitionedCall:0
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'StatefulPartitionedCall:01' shape=[1, 4, 4, 16] dtype=int8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'StatefulPartitionedCall:0' shape=[1, 4, 4, 16] dtype=uint8>


################################################################################
Tensor Allocation for mem_area Sram, of mem_type_set (Scratch, Scratch_fast), using allocator HillClimb, in Cpu and Npu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          3:        0x0 -      0x400:        1024:         3056: FeatureMap  : serving_default_conv2d_input:0
         2 -          5:      0x400 -      0x800:        1024:         3056: FeatureMap  : tfl.quantize
         3 -          5:      0x800 -      0xbf0:        1008:         3056: Weights     : sequential/conv2d/Conv2D_reshape_npu_buffer
         4 -          7:        0x0 -      0x100:         256:         2288: FeatureMap  : StatefulPartitionedCall:01
         6 -          9:      0x100 -      0x200:         256:          512: FeatureMap  : StatefulPartitionedCall:0_cpu
Allocation Peak Tensor Size:       3056 (     0xbf0) Bytes     2.98 KiB
Allocation Peak Memory Usage:      3056 (     0xbf0) Bytes     2.98 KiB
Allocation Overhead:                  0 Bytes (0.00 %)

################################################################################
Tensor Allocation for mem_area OffChipFlash, of mem_type_set (Permanent_NPU), using allocator LinearAlloc, in Npu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          1:        0x0 -       0xa0:         160:         1168: FeatureMap  : sequential/conv2d/BiasAdd/ReadVariableOp_reshape_npu
         0 -          1:       0xa0 -      0x490:        1008:         1168: Weights     : sequential/conv2d/Conv2D_reshape_npu_npu_encoded_weights
Allocation Peak Tensor Size:       1168 (     0x490) Bytes     1.14 KiB
Allocation Peak Memory Usage:      1168 (     0x490) Bytes     1.14 KiB
Allocation Overhead:                  0 Bytes (0.00 %)
print_high_level_command_stream() main_split_1
  0 <NpuStripe: name=tfl.quantize, ifm_box=<Box [0, 0, 0, 0] - [1, 8, 8, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 8, 8, 16]>, weight_box=None, block_config=[6, 6, 16, 16]>
  1 <DMA: name=StatefulPartitionedCall:01, in=sequential/conv2d/Conv2D_reshape_npu_npu_encoded_weights, out=sequential/conv2d/Conv2D_reshape_npu_buffer box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>>
  2 <NpuStripe: name=StatefulPartitionedCall:01, ifm_box=<Box [0, 0, 0, 0] - [1, 8, 8, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 4, 4, 16]>, weight_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, block_config=[4, 4, 16, 16]>
  3 <NpuStripe: name=StatefulPartitionedCall:0, ifm_box=<Box [0, 0, 0, 0] - [1, 4, 4, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 4, 4, 16]>, weight_box=None, block_config=[4, 4, 16, 16]>
Register-Level Command Stream: Input
0 AVERAGE Pooling name=tfl.quantize: <NpuStripe: name=tfl.quantize, ifm_box=<Box [0, 0, 0, 0] - [1, 8, 8, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 8, 8, 16]>, weight_box=None, block_config=[6, 6, 16, 16]>
      IFM: h=8,w=8,c=16, region=1, NHWC, UINT8, size=1024, scale: 0.9999980926513672, zero: 0
         Stride y/x/c: 128/16/1, tiles: w0=8, h0=8, h1=8, base=['0x0', '0x0', '0x0', '0x0']
         name=serving_default_conv2d_input:0_npu
      OFM: h=8,w=8,c=16, region=1, NHCWB16, INT8, size=1024, scale: 0.9999980926513672, zero: -128
         Stride y/x/c: 128/16/128, tiles: w0=8, h0=8, h1=8, base=['0x400', '0x0', '0x0', '0x0']
         name=tfl.quantize
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Block config: h=6,w=6,c=16, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
1 Dma name=sequential/conv2d/Conv2D_reshape_npu_buffer, src=(region=0, address=0xa0, length=1008), dest=(region=1, address=0x800, length=1008): <DMA: name=StatefulPartitionedCall:01, in=sequential/conv2d/Conv2D_reshape_npu_npu_encoded_weights, out=sequential/conv2d/Conv2D_reshape_npu_buffer box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>>
2 Conv2D name=StatefulPartitionedCall:01: <NpuStripe: name=StatefulPartitionedCall:01, ifm_box=<Box [0, 0, 0, 0] - [1, 8, 8, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 4, 4, 16]>, weight_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, block_config=[4, 4, 16, 16]>
      IFM: h=8,w=8,c=16, region=1, NHCWB16, INT8, size=1024, scale: 0.9999980926513672, zero: -128
         Stride y/x/c: 128/16/128, tiles: w0=8, h0=8, h1=8, base=['0x400', '0x0', '0x0', '0x0']
         name=tfl.quantize
      OFM: h=4,w=4,c=16, region=1, NHCWB16, INT8, size=256, scale: 21227.548828125, zero: -128
         Stride y/x/c: 64/16/64, tiles: w0=4, h0=4, h1=4, base=['0x0', '0x0', '0x0', '0x0']
         name=StatefulPartitionedCall:01
      Kernel: w=2, h=2, stride=(2, 2), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Weights: (region=1, address=0x8a0, length=848)
      Scales: (region=1, address=0x800, length=160)
      NpuBlockTraversal.PART_KERNEL_FIRST
      Block config: h=4,w=4,c=16, NpuResamplingMode.NONE, NpuRoundingMode.TFL
3 AVERAGE Pooling name=StatefulPartitionedCall:0: <NpuStripe: name=StatefulPartitionedCall:0, ifm_box=<Box [0, 0, 0, 0] - [1, 4, 4, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 4, 4, 16]>, weight_box=None, block_config=[4, 4, 16, 16]>
      IFM: h=4,w=4,c=16, region=1, NHCWB16, INT8, size=256, scale: 21227.548828125, zero: -128
         Stride y/x/c: 64/16/64, tiles: w0=4, h0=4, h1=4, base=['0x0', '0x0', '0x0', '0x0']
         name=StatefulPartitionedCall:01
      OFM: h=4,w=4,c=16, region=1, NHWC, UINT8, size=256, scale: 21227.548828125, zero: 0
         Stride y/x/c: 64/16/1, tiles: w0=4, h0=4, h1=4, base=['0x100', '0x0', '0x0', '0x0']
         name=StatefulPartitionedCall:0_cpu
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Block config: h=4,w=4,c=16, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
Register-Level Command Stream: Output
  Offset: Payload Param Code - Command                        Param
0x000000:          0001 010f - cmd0.NPU_SET_IFM_REGION            1
0x000004: 00000000 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x00000c: 00000000 0000 4001 - cmd1.NPU_SET_IFM_BASE1             0
0x000014: 00000000 0000 4002 - cmd1.NPU_SET_IFM_BASE2             0
0x00001c: 00000000 0000 4003 - cmd1.NPU_SET_IFM_BASE3             0
0x000024:          0007 010b - cmd0.NPU_SET_IFM_HEIGHT0_M1        7
0x000028:          0007 010c - cmd0.NPU_SET_IFM_HEIGHT1_M1        7
0x00002c:          0007 010a - cmd0.NPU_SET_IFM_WIDTH0_M1         7
0x000030:          000f 0104 - cmd0.NPU_SET_IFM_DEPTH_M1         15
0x000034: 00000001 0000 4006 - cmd1.NPU_SET_IFM_STRIDE_C          0
0x00003c: 00000080 0000 4005 - cmd1.NPU_SET_IFM_STRIDE_Y          0
0x000044: 00000010 0000 4004 - cmd1.NPU_SET_IFM_STRIDE_X          0
0x00004c:          0000 0109 - cmd0.NPU_SET_IFM_ZERO_POINT        0
0x000050:          0000 0105 - cmd0.NPU_SET_IFM_PRECISION         0
0x000054:          0000 0107 - cmd0.NPU_SET_IFM_UPSCALE           0
0x000058:          0000 0100 - cmd0.NPU_SET_IFM_PAD_TOP           0
0x00005c:          0000 0101 - cmd0.NPU_SET_IFM_PAD_LEFT          0
0x000060:          0000 0103 - cmd0.NPU_SET_IFM_PAD_BOTTOM        0
0x000064:          0000 0102 - cmd0.NPU_SET_IFM_PAD_RIGHT         0
0x000068:          0001 011f - cmd0.NPU_SET_OFM_REGION            1
0x00006c: 00000400 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x000074: 00000000 0000 4011 - cmd1.NPU_SET_OFM_BASE1             0
0x00007c: 00000000 0000 4012 - cmd1.NPU_SET_OFM_BASE2             0
0x000084: 00000000 0000 4013 - cmd1.NPU_SET_OFM_BASE3             0
0x00008c:          0007 011b - cmd0.NPU_SET_OFM_HEIGHT0_M1        7
0x000090:          0007 011c - cmd0.NPU_SET_OFM_HEIGHT1_M1        7
0x000094:          0007 011a - cmd0.NPU_SET_OFM_WIDTH0_M1         7
0x000098:          0007 0112 - cmd0.NPU_SET_OFM_HEIGHT_M1         7
0x00009c:          0007 0111 - cmd0.NPU_SET_OFM_WIDTH_M1          7
0x0000a0:          000f 0113 - cmd0.NPU_SET_OFM_DEPTH_M1         15
0x0000a4: 00000080 0000 4016 - cmd1.NPU_SET_OFM_STRIDE_C          0
0x0000ac: 00000080 0000 4015 - cmd1.NPU_SET_OFM_STRIDE_Y          0
0x0000b4: 00000010 0000 4014 - cmd1.NPU_SET_OFM_STRIDE_X          0
0x0000bc:          ff80 0118 - cmd0.NPU_SET_OFM_ZERO_POINT    65408
0x0000c0:          0141 0114 - cmd0.NPU_SET_OFM_PRECISION       321
0x0000c4:          0000 0121 - cmd0.NPU_SET_KERNEL_HEIGHT_M1      0
0x0000c8:          0000 0120 - cmd0.NPU_SET_KERNEL_WIDTH_M1       0
0x0000cc:          0000 0122 - cmd0.NPU_SET_KERNEL_STRIDE         0
0x0000d0:          0000 0125 - cmd0.NPU_SET_ACTIVATION            0
0x0000d4:          ff80 0126 - cmd0.NPU_SET_ACTIVATION_MIN    65408
0x0000d8:          007f 0127 - cmd0.NPU_SET_ACTIVATION_MAX      127
0x0000dc:          0005 0116 - cmd0.NPU_SET_OFM_BLK_HEIGHT_M1     5
0x0000e0:          0005 0115 - cmd0.NPU_SET_OFM_BLK_WIDTH_M1      5
0x0000e4:          000f 0117 - cmd0.NPU_SET_OFM_BLK_DEPTH_M1     15
0x0000e8:          000a 010d - cmd0.NPU_SET_IFM_IB_END           10
0x0000ec:          001e 012d - cmd0.NPU_SET_AB_START             30
0x0000f0:          0000 0124 - cmd0.NPU_SET_ACC_FORMAT            0
0x0000f4: 40000000 001e 4024 - cmd1.NPU_SET_OFM_SCALE            30
0x0000fc:          0000 012f - cmd0.NPU_SET_BLOCKDEP              0
0x000100:          0001 0005 - cmd0.NPU_OP_POOL                   1
0x000104:          0000 0130 - cmd0.NPU_SET_DMA0_SRC_REGION       0
0x000108: 000000a0 0000 4030 - cmd1.NPU_SET_DMA0_SRC              0
0x000110:          0001 0131 - cmd0.NPU_SET_DMA0_DST_REGION       1
0x000114: 00000800 0000 4031 - cmd1.NPU_SET_DMA0_DST              0
0x00011c: 000003f0 0000 4032 - cmd1.NPU_SET_DMA0_LEN              0
0x000124:          0000 0010 - cmd0.NPU_OP_DMA_START              0
0x000128: 00000400 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x000130: 00000080 0000 4006 - cmd1.NPU_SET_IFM_STRIDE_C          0
0x000138:          ff80 0109 - cmd0.NPU_SET_IFM_ZERO_POINT    65408
0x00013c:          0041 0105 - cmd0.NPU_SET_IFM_PRECISION        65
0x000140: 00000000 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x000148:          0003 011b - cmd0.NPU_SET_OFM_HEIGHT0_M1        3
0x00014c:          0003 011c - cmd0.NPU_SET_OFM_HEIGHT1_M1        3
0x000150:          0003 011a - cmd0.NPU_SET_OFM_WIDTH0_M1         3
0x000154:          0003 0112 - cmd0.NPU_SET_OFM_HEIGHT_M1         3
0x000158:          0003 0111 - cmd0.NPU_SET_OFM_WIDTH_M1          3
0x00015c: 00000040 0000 4016 - cmd1.NPU_SET_OFM_STRIDE_C          0
0x000164: 00000040 0000 4015 - cmd1.NPU_SET_OFM_STRIDE_Y          0
0x00016c:          0041 0114 - cmd0.NPU_SET_OFM_PRECISION        65
0x000170:          0001 0121 - cmd0.NPU_SET_KERNEL_HEIGHT_M1      1
0x000174:          0001 0120 - cmd0.NPU_SET_KERNEL_WIDTH_M1       1
0x000178:          0007 0122 - cmd0.NPU_SET_KERNEL_STRIDE         7
0x00017c:          0001 0128 - cmd0.NPU_SET_WEIGHT_REGION         1
0x000180: 000008a0 0000 4020 - cmd1.NPU_SET_WEIGHT_BASE           0
0x000188: 00000350 0000 4021 - cmd1.NPU_SET_WEIGHT_LENGTH         0
0x000190:          0001 0129 - cmd0.NPU_SET_SCALE_REGION          1
0x000194: 00000800 0000 4022 - cmd1.NPU_SET_SCALE_BASE            0
0x00019c: 000000a0 0000 4023 - cmd1.NPU_SET_SCALE_LENGTH          0
0x0001a4:          0003 0116 - cmd0.NPU_SET_OFM_BLK_HEIGHT_M1     3
0x0001a8:          0003 0115 - cmd0.NPU_SET_OFM_BLK_WIDTH_M1      3
0x0001ac:          0000 0011 - cmd0.NPU_OP_DMA_WAIT               0
0x0001b0:          0000 0002 - cmd0.NPU_OP_CONV                   0
0x0001b4: 00000000 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x0001bc:          0003 010b - cmd0.NPU_SET_IFM_HEIGHT0_M1        3
0x0001c0:          0003 010c - cmd0.NPU_SET_IFM_HEIGHT1_M1        3
0x0001c4:          0003 010a - cmd0.NPU_SET_IFM_WIDTH0_M1         3
0x0001c8: 00000040 0000 4006 - cmd1.NPU_SET_IFM_STRIDE_C          0
0x0001d0: 00000040 0000 4005 - cmd1.NPU_SET_IFM_STRIDE_Y          0
0x0001d8: 00000100 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x0001e0: 00000001 0000 4016 - cmd1.NPU_SET_OFM_STRIDE_C          0
0x0001e8:          0000 0118 - cmd0.NPU_SET_OFM_ZERO_POINT        0
0x0001ec:          0100 0114 - cmd0.NPU_SET_OFM_PRECISION       256
0x0001f0:          0000 0121 - cmd0.NPU_SET_KERNEL_HEIGHT_M1      0
0x0001f4:          0000 0120 - cmd0.NPU_SET_KERNEL_WIDTH_M1       0
0x0001f8:          0000 0122 - cmd0.NPU_SET_KERNEL_STRIDE         0
0x0001fc:          0000 0126 - cmd0.NPU_SET_ACTIVATION_MIN        0
0x000200:          00ff 0127 - cmd0.NPU_SET_ACTIVATION_MAX      255
0x000204:          0001 0005 - cmd0.NPU_OP_POOL                   1
0x000208:          ffff 0000 - cmd0.NPU_OP_STOP               65535
Number of commands = 99
Command stream length = 524 bytes

################################################################################
Tensor Allocation for mem_area OffChipFlash, of mem_type_set (Permanent_CPU), using allocator LinearAlloc, in Cpu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          3:        0x0 -      0x230:         560:         1728: FeatureMap  : main_split_1_command_stream
         0 -          3:      0x230 -      0x6c0:        1168:         1728: FeatureMap  : main_split_1_flash
Allocation Peak Tensor Size:       1728 (     0x6c0) Bytes     1.69 KiB
Allocation Peak Memory Usage:      1728 (     0x6c0) Bytes     1.69 KiB
Allocation Overhead:                  0 Bytes (0.00 %)

Network summary for tflite_model
Accelerator configuration               Ethos_U55_256
System configuration             Ethos_U55_High_End_Embedded
Memory mode                               Shared_Sram
Accelerator clock                                 500 MHz
Design peak SRAM bandwidth                       3.73 GB/s
Design peak Off-chip Flash bandwidth             0.47 GB/s

Total SRAM used                                  2.98 KiB
Total Off-chip Flash used                        1.69 KiB

CPU operators = 0 (0.0%)
NPU operators = 3 (100.0%)

Average SRAM bandwidth                           2.37 GB/s
Input   SRAM bandwidth                           0.00 MB/batch
Weight  SRAM bandwidth                           0.00 MB/batch
Output  SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth                           0.01 MB/batch
Total   SRAM bandwidth            per input      0.01 MB/inference (batch size 1)

Average Off-chip Flash bandwidth                 0.33 GB/s
Input   Off-chip Flash bandwidth                 0.00 MB/batch
Weight  Off-chip Flash bandwidth                 0.00 MB/batch
Output  Off-chip Flash bandwidth                 0.00 MB/batch
Total   Off-chip Flash bandwidth                 0.00 MB/batch
Total   Off-chip Flash bandwidth  per input      0.00 MB/inference (batch size 1)

Original Weights Size                            1.00 KiB
NPU Encoded Weights Size                         0.83 KiB

Neural network macs                             17664 MACs/batch

Illustrate Vela Compiled Model in Netron? (y/n)
++ Converting tflite_model_vela.tflite to    tflite_model_vela.tflite.cc
