Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 16)]                 0         []                            
                                                                                                  
 input_2 (InputLayer)        [(None, 16)]                 0         []                            
                                                                                                  
 add (Add)                   (None, 16)                   0         ['input_1[0][0]',             
                                                                     'input_2[0][0]']             
                                                                                                  
==================================================================================================
Total params: 0 (0.00 Byte)
Trainable params: 0 (0.00 Byte)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
IFM1: [[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]]
IFM2: [[16 15 14 13 12 11 10  9  8  7  6  5  4  3  2  1]]
Output: [[17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17.]]
signatures _SignatureMap({'serving_default': <ConcreteFunction (*, input_1: TensorSpec(shape=(None, 16), dtype=tf.float32, name='input_1'), input_2: TensorSpec(shape=(None, 16), dtype=tf.float32, name='input_2')) -> Dict[['add', TensorSpec(shape=(None, 16), dtype=tf.float32, name='add')]] at 0x7F395C646AD0>})
Quantized TFLite model saved at: saved_models/tflite_model/tflite_model.tflite
Warning: No configuration file specified. Using a default of ['/home/chris/.local/lib/python3.10/site-packages/ethosu/config_files/Arm/vela.ini']. Compilation may be invalid or non-optimal.
Warning: No system configuration specified. Using a default of Ethos_U55_High_End_Embedded. Compilation may be invalid or non-optimal.
Warning: No memory mode specified. Using a default of Shared_Sram. Compilation may be invalid or non-optimal.
Configuration files:
   original = None
   used = ['/home/chris/.local/lib/python3.10/site-packages/ethosu/config_files/Arm/vela.ini']
System Configuration (Ethos_U55_High_End_Embedded):
   core_clock = 500000000.0
   axi0_port = Sram
   axi1_port = OffChipFlash
   Sram_clock_scales = 1.0
   Sram_burst_length = 32
   Sram_read_latency = 32
   Sram_write_latency = 32
   Dram_clock_scales = 1.0
   Dram_burst_length = 1
   Dram_read_latency = 0
   Dram_write_latency = 0
   OnChipFlash_clock_scales = 1.0
   OnChipFlash_burst_length = 1
   OnChipFlash_read_latency = 0
   OnChipFlash_write_latency = 0
   OffChipFlash_clock_scales = 0.125
   OffChipFlash_burst_length = 128
   OffChipFlash_read_latency = 64
   OffChipFlash_write_latency = 0
Memory Mode (Shared_Sram):
   const_mem_area = Axi1
   arena_mem_area = Axi0
   cache_mem_area = Axi0
   arena_cache_size = 4294967296 from Default
Architecture Settings:
   permanent_storage_mem_area = OffChipFlash
   feature_map_storage_mem_area = Sram
   fast_storage_mem_area = Sram
Operators of Subgraph main
  0: Quantize - {'attribute_read_error': []} - tfl.quantize
  1: Quantize - {'attribute_read_error': []} - tfl.quantize1
  2: Add      - {'attribute_read_error': [], 'fused_activation_function': None, 'pot_scale_int16': True} - PartitionedCall:01
  3: Quantize - {'attribute_read_error': []} - PartitionedCall:0
print_graph_with_tensors() main
0 Placeholder serving_default_input_1:0
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_input_1:0' shape=[1, 16] dtype=uint8>

1 AvgPool tfl.quantize
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_input_1:0' shape=[1, 16] dtype=uint8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize' shape=[1, 16] dtype=int8>

2 Placeholder serving_default_input_2:0
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_input_2:0' shape=[1, 16] dtype=uint8>

3 AvgPool tfl.quantize1
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_input_2:0' shape=[1, 16] dtype=uint8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize1' shape=[1, 16] dtype=int8>

4 Add PartitionedCall:01
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize' shape=[1, 16] dtype=int8>
    Input  01           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize1' shape=[1, 16] dtype=int8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'PartitionedCall:01' shape=[1, 16] dtype=int8>

5 AvgPool PartitionedCall:0
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'PartitionedCall:01' shape=[1, 16] dtype=int8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'PartitionedCall:0' shape=[1, 16] dtype=uint8>


################################################################################
Tensor Allocation for mem_area Sram, of mem_type_set (Scratch, Scratch_fast), using allocator HillClimb, in Cpu and Npu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          3:       0x20 -       0x30:          16:           48: FeatureMap  : serving_default_input_2:0
         0 -          5:        0x0 -       0x10:          16:           48: FeatureMap  : serving_default_input_1:0
         2 -          7:       0x10 -       0x20:          16:           48: FeatureMap  : tfl.quantize1
         4 -          9:       0x20 -       0x30:          16:           48: FeatureMap  : PartitionedCall:01
         4 -          9:       0x20 -       0x30:          16:           48: FeatureMap  : tfl.quantize
         8 -         11:        0x0 -       0x10:          16:           32: FeatureMap  : PartitionedCall:0_cpu
Allocation Peak Tensor Size:         48 (      0x30) Bytes     0.05 KiB
Allocation Peak Memory Usage:        48 (      0x30) Bytes     0.05 KiB
Allocation Overhead:                  0 Bytes (0.00 %)
print_high_level_command_stream() main_split_1
  0 <NpuStripe: name=tfl.quantize1, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
  1 <NpuStripe: name=tfl.quantize, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
  2 <NpuStripe: name=PartitionedCall:01, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
  3 <NpuStripe: name=PartitionedCall:0, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
Register-Level Command Stream: Input
0 AVERAGE Pooling name=tfl.quantize1: <NpuStripe: name=tfl.quantize1, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
      IFM: h=1,w=1,c=16, region=1, NHWC, UINT8, size=16, scale: 0.999677300453186, zero: 0
         Stride y/x/c: 16/16/1, tiles: w0=1, h0=1, h1=1, base=['0x20', '0x0', '0x0', '0x0']
         name=serving_default_input_2:0_npu
      OFM: h=1,w=1,c=16, region=1, NHCWB16, INT8, size=16, scale: 0.999677300453186, zero: -128
         Stride y/x/c: 16/16/16, tiles: w0=1, h0=1, h1=1, base=['0x10', '0x0', '0x0', '0x0']
         name=tfl.quantize1
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Block config: h=2,w=2,c=16, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
1 AVERAGE Pooling name=tfl.quantize: <NpuStripe: name=tfl.quantize, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
      IFM: h=1,w=1,c=16, region=1, NHWC, UINT8, size=16, scale: 0.9997864961624146, zero: 0
         Stride y/x/c: 16/16/1, tiles: w0=1, h0=1, h1=1, base=['0x0', '0x0', '0x0', '0x0']
         name=serving_default_input_1:0_npu
      OFM: h=1,w=1,c=16, region=1, NHCWB16, INT8, size=16, scale: 0.9997864961624146, zero: -128
         Stride y/x/c: 16/16/16, tiles: w0=1, h0=1, h1=1, base=['0x20', '0x0', '0x0', '0x0']
         name=tfl.quantize
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Block config: h=2,w=2,c=16, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
2 ADD ElementWise name=PartitionedCall:01: <NpuStripe: name=PartitionedCall:01, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
      IFM: h=1,w=1,c=16, region=1, NHCWB16, INT8, size=16, scale: 0.9997864961624146, zero: -128
         Stride y/x/c: 16/16/16, tiles: w0=1, h0=1, h1=1, base=['0x20', '0x0', '0x0', '0x0']
         name=tfl.quantize
      IFM2: h=1,w=1,c=16, region=1, NHCWB16, INT8, size=16, scale: 0.999677300453186, zero: -128
         Stride y/x/c: 16/16/16, tiles: w0=1, h0=1, h1=1, base=['0x10', '0x0', '0x0', '0x0']
         name=tfl.quantize1
      OFM: h=1,w=1,c=16, region=1, NHCWB16, INT8, size=16, scale: 1.945576548576355, zero: -128
         Stride y/x/c: 16/16/16, tiles: w0=1, h0=1, h1=1, base=['0x20', '0x0', '0x0', '0x0']
         name=PartitionedCall:01
      Block config: h=2,w=2,c=16, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
3 AVERAGE Pooling name=PartitionedCall:0: <NpuStripe: name=PartitionedCall:0, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 16]>, weight_box=None, block_config=[2, 2, 16, 16]>
      IFM: h=1,w=1,c=16, region=1, NHCWB16, INT8, size=16, scale: 1.945576548576355, zero: -128
         Stride y/x/c: 16/16/16, tiles: w0=1, h0=1, h1=1, base=['0x20', '0x0', '0x0', '0x0']
         name=PartitionedCall:01
      OFM: h=1,w=1,c=16, region=1, NHWC, UINT8, size=16, scale: 1.945576548576355, zero: 0
         Stride y/x/c: 16/16/1, tiles: w0=1, h0=1, h1=1, base=['0x0', '0x0', '0x0', '0x0']
         name=PartitionedCall:0_cpu
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Block config: h=2,w=2,c=16, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
Register-Level Command Stream: Output
  Offset: Payload Param Code - Command                        Param
0x000000:          0001 010f - cmd0.NPU_SET_IFM_REGION            1
0x000004: 00000020 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x00000c: 00000000 0000 4001 - cmd1.NPU_SET_IFM_BASE1             0
0x000014: 00000000 0000 4002 - cmd1.NPU_SET_IFM_BASE2             0
0x00001c: 00000000 0000 4003 - cmd1.NPU_SET_IFM_BASE3             0
0x000024:          0000 010b - cmd0.NPU_SET_IFM_HEIGHT0_M1        0
0x000028:          0000 010c - cmd0.NPU_SET_IFM_HEIGHT1_M1        0
0x00002c:          0000 010a - cmd0.NPU_SET_IFM_WIDTH0_M1         0
0x000030:          000f 0104 - cmd0.NPU_SET_IFM_DEPTH_M1         15
0x000034: 00000001 0000 4006 - cmd1.NPU_SET_IFM_STRIDE_C          0
0x00003c: 00000010 0000 4005 - cmd1.NPU_SET_IFM_STRIDE_Y          0
0x000044: 00000010 0000 4004 - cmd1.NPU_SET_IFM_STRIDE_X          0
0x00004c:          0000 0109 - cmd0.NPU_SET_IFM_ZERO_POINT        0
0x000050:          0000 0105 - cmd0.NPU_SET_IFM_PRECISION         0
0x000054:          0000 0107 - cmd0.NPU_SET_IFM_UPSCALE           0
0x000058:          0000 0100 - cmd0.NPU_SET_IFM_PAD_TOP           0
0x00005c:          0000 0101 - cmd0.NPU_SET_IFM_PAD_LEFT          0
0x000060:          0000 0103 - cmd0.NPU_SET_IFM_PAD_BOTTOM        0
0x000064:          0000 0102 - cmd0.NPU_SET_IFM_PAD_RIGHT         0
0x000068:          0001 011f - cmd0.NPU_SET_OFM_REGION            1
0x00006c: 00000010 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x000074: 00000000 0000 4011 - cmd1.NPU_SET_OFM_BASE1             0
0x00007c: 00000000 0000 4012 - cmd1.NPU_SET_OFM_BASE2             0
0x000084: 00000000 0000 4013 - cmd1.NPU_SET_OFM_BASE3             0
0x00008c:          0000 011b - cmd0.NPU_SET_OFM_HEIGHT0_M1        0
0x000090:          0000 011c - cmd0.NPU_SET_OFM_HEIGHT1_M1        0
0x000094:          0000 011a - cmd0.NPU_SET_OFM_WIDTH0_M1         0
0x000098:          0000 0112 - cmd0.NPU_SET_OFM_HEIGHT_M1         0
0x00009c:          0000 0111 - cmd0.NPU_SET_OFM_WIDTH_M1          0
0x0000a0:          000f 0113 - cmd0.NPU_SET_OFM_DEPTH_M1         15
0x0000a4: 00000010 0000 4016 - cmd1.NPU_SET_OFM_STRIDE_C          0
0x0000ac: 00000010 0000 4015 - cmd1.NPU_SET_OFM_STRIDE_Y          0
0x0000b4: 00000010 0000 4014 - cmd1.NPU_SET_OFM_STRIDE_X          0
0x0000bc:          ff80 0118 - cmd0.NPU_SET_OFM_ZERO_POINT    65408
0x0000c0:          0141 0114 - cmd0.NPU_SET_OFM_PRECISION       321
0x0000c4:          0000 0121 - cmd0.NPU_SET_KERNEL_HEIGHT_M1      0
0x0000c8:          0000 0120 - cmd0.NPU_SET_KERNEL_WIDTH_M1       0
0x0000cc:          0000 0122 - cmd0.NPU_SET_KERNEL_STRIDE         0
0x0000d0:          0000 0125 - cmd0.NPU_SET_ACTIVATION            0
0x0000d4:          ff80 0126 - cmd0.NPU_SET_ACTIVATION_MIN    65408
0x0000d8:          007f 0127 - cmd0.NPU_SET_ACTIVATION_MAX      127
0x0000dc:          0001 0116 - cmd0.NPU_SET_OFM_BLK_HEIGHT_M1     1
0x0000e0:          0001 0115 - cmd0.NPU_SET_OFM_BLK_WIDTH_M1      1
0x0000e4:          000f 0117 - cmd0.NPU_SET_OFM_BLK_DEPTH_M1     15
0x0000e8:          000a 010d - cmd0.NPU_SET_IFM_IB_END           10
0x0000ec:          001e 012d - cmd0.NPU_SET_AB_START             30
0x0000f0:          0000 0124 - cmd0.NPU_SET_ACC_FORMAT            0
0x0000f4: 40000000 001e 4024 - cmd1.NPU_SET_OFM_SCALE            30
0x0000fc:          0000 012f - cmd0.NPU_SET_BLOCKDEP              0
0x000100:          0001 0005 - cmd0.NPU_OP_POOL                   1
0x000104: 00000000 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x00010c: 00000020 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x000114:          0003 012f - cmd0.NPU_SET_BLOCKDEP              3
0x000118:          0001 0005 - cmd0.NPU_OP_POOL                   1
0x00011c: 7ffc6bce 000c 4025 - cmd1.NPU_SET_OPA_SCALE            12
0x000124: 00000000 0000 4026 - cmd1.NPU_SET_OPB_SCALE             0
0x00012c: 41c6b65e 0032 4024 - cmd1.NPU_SET_OFM_SCALE            50
0x000134: 00000020 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x00013c: 00000010 0000 4006 - cmd1.NPU_SET_IFM_STRIDE_C          0
0x000144:          ff80 0109 - cmd0.NPU_SET_IFM_ZERO_POINT    65408
0x000148:          0241 0105 - cmd0.NPU_SET_IFM_PRECISION       577
0x00014c:          002e 010d - cmd0.NPU_SET_IFM_IB_END           46
0x000150:          002e 012d - cmd0.NPU_SET_AB_START             46
0x000154:          000a 018d - cmd0.NPU_SET_IFM2_IB_START        10
0x000158:          0001 018f - cmd0.NPU_SET_IFM2_REGION           1
0x00015c: 00000010 0000 4080 - cmd1.NPU_SET_IFM2_BASE0            0
0x000164: 00000000 0000 4081 - cmd1.NPU_SET_IFM2_BASE1            0
0x00016c: 00000000 0000 4082 - cmd1.NPU_SET_IFM2_BASE2            0
0x000174: 00000000 0000 4083 - cmd1.NPU_SET_IFM2_BASE3            0
0x00017c:          0000 018b - cmd0.NPU_SET_IFM2_HEIGHT0_M1       0
0x000180:          0000 018c - cmd0.NPU_SET_IFM2_HEIGHT1_M1       0
0x000184:          0000 018a - cmd0.NPU_SET_IFM2_WIDTH0_M1        0
0x000188: 00000010 0000 4086 - cmd1.NPU_SET_IFM2_STRIDE_C         0
0x000190: 00000010 0000 4085 - cmd1.NPU_SET_IFM2_STRIDE_Y         0
0x000198: 00000010 0000 4084 - cmd1.NPU_SET_IFM2_STRIDE_X         0
0x0001a0:          ff80 0189 - cmd0.NPU_SET_IFM2_ZERO_POINT   65408
0x0001a4:          0041 0185 - cmd0.NPU_SET_IFM2_PRECISION       65
0x0001a8:          0000 0180 - cmd0.NPU_SET_IFM2_BROADCAST        0
0x0001ac:          0000 012f - cmd0.NPU_SET_BLOCKDEP              0
0x0001b0:          0001 0006 - cmd0.NPU_OP_ELEMENTWISE            1
0x0001b4:          0041 0105 - cmd0.NPU_SET_IFM_PRECISION        65
0x0001b8: 00000000 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x0001c0: 00000001 0000 4016 - cmd1.NPU_SET_OFM_STRIDE_C          0
0x0001c8:          0000 0118 - cmd0.NPU_SET_OFM_ZERO_POINT        0
0x0001cc:          0100 0114 - cmd0.NPU_SET_OFM_PRECISION       256
0x0001d0:          0000 0126 - cmd0.NPU_SET_ACTIVATION_MIN        0
0x0001d4:          00ff 0127 - cmd0.NPU_SET_ACTIVATION_MAX      255
0x0001d8:          000a 010d - cmd0.NPU_SET_IFM_IB_END           10
0x0001dc:          001e 012d - cmd0.NPU_SET_AB_START             30
0x0001e0: 40000000 001e 4024 - cmd1.NPU_SET_OFM_SCALE            30
0x0001e8:          0001 0005 - cmd0.NPU_OP_POOL                   1
0x0001ec:          ffff 0000 - cmd0.NPU_OP_STOP               65535
Number of commands = 92
Command stream length = 496 bytes

################################################################################
Tensor Allocation for mem_area OffChipFlash, of mem_type_set (Permanent_CPU), using allocator LinearAlloc, in Cpu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          3:      0x210 -      0x220:          16:          544: FeatureMap  : main_split_1_flash
         0 -          3:        0x0 -      0x210:         528:          544: FeatureMap  : main_split_1_command_stream
Allocation Peak Tensor Size:        544 (     0x220) Bytes     0.53 KiB
Allocation Peak Memory Usage:       544 (     0x220) Bytes     0.53 KiB
Allocation Overhead:                  0 Bytes (0.00 %)

Network summary for tflite_model
Accelerator configuration               Ethos_U55_256
System configuration             Ethos_U55_High_End_Embedded
Memory mode                               Shared_Sram
Accelerator clock                                 500 MHz
Design peak SRAM bandwidth                       3.73 GB/s

Total SRAM used                                  0.05 KiB

CPU operators = 0 (0.0%)
NPU operators = 4 (100.0%)

Average SRAM bandwidth                           0.51 GB/s
Input   SRAM bandwidth                           0.00 MB/batch
Weight  SRAM bandwidth                           0.00 MB/batch
Output  SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth            per input      0.00 MB/inference (batch size 1)

Original Weights Size                            0.00 KiB
NPU Encoded Weights Size                         0.00 KiB

Neural network macs                                48 MACs/batch

Illustrate Vela Compiled Model in Netron? (y/n)
++ Converting tflite_model_vela.tflite to    tflite_model_vela.tflite.cc
