âœ… Fully Quantized TFLite model saved at: saved_models/tflite_model/tflite_model.tflite
Warning: No configuration file specified. Using a default of ['/home/chris/.local/lib/python3.10/site-packages/ethosu/config_files/Arm/vela.ini']. Compilation may be invalid or non-optimal.
Warning: No system configuration specified. Using a default of Ethos_U55_High_End_Embedded. Compilation may be invalid or non-optimal.
Warning: No memory mode specified. Using a default of Shared_Sram. Compilation may be invalid or non-optimal.
Configuration files:
   original = None
   used = ['/home/chris/.local/lib/python3.10/site-packages/ethosu/config_files/Arm/vela.ini']
System Configuration (Ethos_U55_High_End_Embedded):
   core_clock = 500000000.0
   axi0_port = Sram
   axi1_port = OffChipFlash
   Sram_clock_scales = 1.0
   Sram_burst_length = 32
   Sram_read_latency = 32
   Sram_write_latency = 32
   Dram_clock_scales = 1.0
   Dram_burst_length = 1
   Dram_read_latency = 0
   Dram_write_latency = 0
   OnChipFlash_clock_scales = 1.0
   OnChipFlash_burst_length = 1
   OnChipFlash_read_latency = 0
   OnChipFlash_write_latency = 0
   OffChipFlash_clock_scales = 0.125
   OffChipFlash_burst_length = 128
   OffChipFlash_read_latency = 64
   OffChipFlash_write_latency = 0
Memory Mode (Shared_Sram):
   const_mem_area = Axi1
   arena_mem_area = Axi0
   cache_mem_area = Axi0
   arena_cache_size = 4294967296 from Default
Architecture Settings:
   permanent_storage_mem_area = OffChipFlash
   feature_map_storage_mem_area = Sram
   fast_storage_mem_area = Sram
Operators of Subgraph main
  0: Quantize       - {'attribute_read_error': []} - tfl.quantize
  1: FullyConnected - {'attribute_read_error': [], 'asymmetric_quantize_inputs': False, 'fused_activation_function': None, 'keep_num_dims': False, 'weights_format': 0, 'quantized_bias_type': 0} - StatefulPartitionedCall:01
  2: Quantize       - {'attribute_read_error': []} - StatefulPartitionedCall:0
print_graph_with_tensors() main
0 Placeholder serving_default_input_1:0
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_input_1:0' shape=[1, 784] dtype=uint8>

1 AvgPool tfl.quantize
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'serving_default_input_1:0' shape=[1, 784] dtype=uint8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize' shape=[1, 784] dtype=int8>

2 Const sequential/dense/MatMul1_reshape
    Output 00              Weights         OffChipFlash        Permanent_NPU <nng.Tensor 'sequential/dense/MatMul1_reshape' shape=[784, 1008] dtype=int8>

3 Const StatefulPartitionedCall:01_bias
    Output 00           FeatureMap         OffChipFlash        Permanent_NPU <nng.Tensor 'StatefulPartitionedCall:01_bias_0' shape=[1008] dtype=int32>

4 FullyConnected StatefulPartitionedCall:01
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'tfl.quantize' shape=[1, 784] dtype=int8>
    Input  01              Weights         OffChipFlash        Permanent_NPU <nng.Tensor 'sequential/dense/MatMul1_reshape' shape=[784, 1008] dtype=int8>
    Input  02           FeatureMap         OffChipFlash        Permanent_NPU <nng.Tensor 'StatefulPartitionedCall:01_bias_0' shape=[1008] dtype=int32>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'StatefulPartitionedCall:01' shape=[1, 1008] dtype=int8>

5 AvgPool StatefulPartitionedCall:0
    Input  00           FeatureMap                 Sram              Scratch <nng.Tensor 'StatefulPartitionedCall:01' shape=[1, 1008] dtype=int8>
    Output 00           FeatureMap                 Sram              Scratch <nng.Tensor 'StatefulPartitionedCall:0' shape=[1, 1008] dtype=uint8>


################################################################################
Tensor Allocation for mem_area Sram, of mem_type_set (Scratch, Scratch_fast), using allocator HillClimb, in Cpu and Npu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          3:        0x0 -      0x310:         784:         1568: FeatureMap  : serving_default_input_1:0
         2 -          5:      0x3f0 -      0x700:         784:         1792: FeatureMap  : tfl.quantize
         4 -          7:        0x0 -      0x3f0:        1008:         2016: FeatureMap  : StatefulPartitionedCall:01
         6 -          9:      0x3f0 -      0x7e0:        1008:         2016: FeatureMap  : StatefulPartitionedCall:0_cpu
Allocation Peak Tensor Size:       2016 (     0x7e0) Bytes     1.97 KiB
Allocation Peak Memory Usage:      2016 (     0x7e0) Bytes     1.97 KiB
Allocation Overhead:                  0 Bytes (0.00 %)

################################################################################
Tensor Allocation for mem_area OffChipFlash, of mem_type_set (Permanent_NPU), using allocator LinearAlloc, in Npu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          1:        0x0 -     0x2760:       10080:       909232: FeatureMap  : StatefulPartitionedCall:01_bias_0_npu
         0 -          1:     0x2760 -    0xddfb0:      899152:       909232: Weights     : sequential/dense/MatMul1_reshape_npu_npu_encoded_weights
Allocation Peak Tensor Size:     909232 (   0xddfb0) Bytes   887.92 KiB
Allocation Peak Memory Usage:    909232 (   0xddfb0) Bytes   887.92 KiB
Allocation Overhead:                  0 Bytes (0.00 %)
print_high_level_command_stream() main_split_1
  0 <NpuStripe: name=tfl.quantize, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 784]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 784]>, weight_box=None, block_config=[2, 2, 128, 128]>
  1 <NpuStripe: name=StatefulPartitionedCall:01, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 784]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, weight_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, block_config=[2, 2, 32, 128]>
  2 <NpuStripe: name=StatefulPartitionedCall:0, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, weight_box=None, block_config=[2, 2, 128, 128]>
Register-Level Command Stream: Input
0 AVERAGE Pooling name=tfl.quantize: <NpuStripe: name=tfl.quantize, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 784]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 784]>, weight_box=None, block_config=[2, 2, 128, 128]>
      IFM: h=1,w=1,c=784, region=1, NHWC, UINT8, size=784, scale: 1.0, zero: 0
         Stride y/x/c: 784/784/1, tiles: w0=1, h0=1, h1=1, base=['0x0', '0x0', '0x0', '0x0']
         name=serving_default_input_1:0_npu
      OFM: h=1,w=1,c=784, region=1, NHCWB16, INT8, size=784, scale: 1.0, zero: -128
         Stride y/x/c: 784/16/16, tiles: w0=1, h0=1, h1=1, base=['0x3f0', '0x0', '0x0', '0x0']
         name=tfl.quantize
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Block config: h=2,w=2,c=128, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
1 FullyConnected Conv2D name=StatefulPartitionedCall:01: <NpuStripe: name=StatefulPartitionedCall:01, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 784]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, weight_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, block_config=[2, 2, 32, 128]>
      IFM: h=1,w=1,c=784, region=1, NHCWB16, INT8, size=784, scale: 1.0, zero: -128
         Stride y/x/c: 784/16/16, tiles: w0=1, h0=1, h1=1, base=['0x3f0', '0x0', '0x0', '0x0']
         name=tfl.quantize
      OFM: h=1,w=1,c=1008, region=1, NHCWB16, INT8, size=1008, scale: 4.634134292602539, zero: -3
         Stride y/x/c: 1008/16/16, tiles: w0=1, h0=1, h1=1, base=['0x0', '0x0', '0x0', '0x0']
         name=StatefulPartitionedCall:01
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Weights: (region=0, address=0x4ec0, length=889072)
      Scales: (region=0, address=0x2760, length=10080)
      NpuBlockTraversal.DEPTH_FIRST
      Block config: h=2,w=2,c=128, NpuResamplingMode.NONE, NpuRoundingMode.TFL
2 AVERAGE Pooling name=StatefulPartitionedCall:0: <NpuStripe: name=StatefulPartitionedCall:0, ifm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, ifm2_box=<Box [] - []>, ofm_box=<Box [0, 0, 0, 0] - [1, 1, 1, 1008]>, weight_box=None, block_config=[2, 2, 128, 128]>
      IFM: h=1,w=1,c=1008, region=1, NHCWB16, INT8, size=1008, scale: 4.634134292602539, zero: -3
         Stride y/x/c: 1008/16/16, tiles: w0=1, h0=1, h1=1, base=['0x0', '0x0', '0x0', '0x0']
         name=StatefulPartitionedCall:01
      OFM: h=1,w=1,c=1008, region=1, NHWC, UINT8, size=1008, scale: 4.634134292602539, zero: 125
         Stride y/x/c: 1008/1008/1, tiles: w0=1, h0=1, h1=1, base=['0x3f0', '0x0', '0x0', '0x0']
         name=StatefulPartitionedCall:0_cpu
      Kernel: w=1, h=1, stride=(1, 1), dilation=(1, 1)
      NpuPadding(top=0, left=0, bottom=0, right=0)
      Block config: h=2,w=2,c=128, NpuResamplingMode.NONE, NpuRoundingMode.TFL, rescale=None
Register-Level Command Stream: Output
  Offset: Payload Param Code - Command                        Param
0x000000:          0001 010f - cmd0.NPU_SET_IFM_REGION            1
0x000004: 00000000 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x00000c: 00000000 0000 4001 - cmd1.NPU_SET_IFM_BASE1             0
0x000014: 00000000 0000 4002 - cmd1.NPU_SET_IFM_BASE2             0
0x00001c: 00000000 0000 4003 - cmd1.NPU_SET_IFM_BASE3             0
0x000024:          0000 010b - cmd0.NPU_SET_IFM_HEIGHT0_M1        0
0x000028:          0000 010c - cmd0.NPU_SET_IFM_HEIGHT1_M1        0
0x00002c:          0000 010a - cmd0.NPU_SET_IFM_WIDTH0_M1         0
0x000030:          030f 0104 - cmd0.NPU_SET_IFM_DEPTH_M1        783
0x000034: 00000001 0000 4006 - cmd1.NPU_SET_IFM_STRIDE_C          0
0x00003c: 00000310 0000 4005 - cmd1.NPU_SET_IFM_STRIDE_Y          0
0x000044: 00000310 0000 4004 - cmd1.NPU_SET_IFM_STRIDE_X          0
0x00004c:          0000 0109 - cmd0.NPU_SET_IFM_ZERO_POINT        0
0x000050:          0000 0105 - cmd0.NPU_SET_IFM_PRECISION         0
0x000054:          0000 0107 - cmd0.NPU_SET_IFM_UPSCALE           0
0x000058:          0000 0100 - cmd0.NPU_SET_IFM_PAD_TOP           0
0x00005c:          0000 0101 - cmd0.NPU_SET_IFM_PAD_LEFT          0
0x000060:          0000 0103 - cmd0.NPU_SET_IFM_PAD_BOTTOM        0
0x000064:          0000 0102 - cmd0.NPU_SET_IFM_PAD_RIGHT         0
0x000068:          0001 011f - cmd0.NPU_SET_OFM_REGION            1
0x00006c: 000003f0 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x000074: 00000000 0000 4011 - cmd1.NPU_SET_OFM_BASE1             0
0x00007c: 00000000 0000 4012 - cmd1.NPU_SET_OFM_BASE2             0
0x000084: 00000000 0000 4013 - cmd1.NPU_SET_OFM_BASE3             0
0x00008c:          0000 011b - cmd0.NPU_SET_OFM_HEIGHT0_M1        0
0x000090:          0000 011c - cmd0.NPU_SET_OFM_HEIGHT1_M1        0
0x000094:          0000 011a - cmd0.NPU_SET_OFM_WIDTH0_M1         0
0x000098:          0000 0112 - cmd0.NPU_SET_OFM_HEIGHT_M1         0
0x00009c:          0000 0111 - cmd0.NPU_SET_OFM_WIDTH_M1          0
0x0000a0:          030f 0113 - cmd0.NPU_SET_OFM_DEPTH_M1        783
0x0000a4: 00000010 0000 4016 - cmd1.NPU_SET_OFM_STRIDE_C          0
0x0000ac: 00000310 0000 4015 - cmd1.NPU_SET_OFM_STRIDE_Y          0
0x0000b4: 00000010 0000 4014 - cmd1.NPU_SET_OFM_STRIDE_X          0
0x0000bc:          ff80 0118 - cmd0.NPU_SET_OFM_ZERO_POINT    65408
0x0000c0:          0141 0114 - cmd0.NPU_SET_OFM_PRECISION       321
0x0000c4:          0000 0121 - cmd0.NPU_SET_KERNEL_HEIGHT_M1      0
0x0000c8:          0000 0120 - cmd0.NPU_SET_KERNEL_WIDTH_M1       0
0x0000cc:          0000 0122 - cmd0.NPU_SET_KERNEL_STRIDE         0
0x0000d0:          0000 0125 - cmd0.NPU_SET_ACTIVATION            0
0x0000d4:          ff80 0126 - cmd0.NPU_SET_ACTIVATION_MIN    65408
0x0000d8:          007f 0127 - cmd0.NPU_SET_ACTIVATION_MAX      127
0x0000dc:          0001 0116 - cmd0.NPU_SET_OFM_BLK_HEIGHT_M1     1
0x0000e0:          0001 0115 - cmd0.NPU_SET_OFM_BLK_WIDTH_M1      1
0x0000e4:          007f 0117 - cmd0.NPU_SET_OFM_BLK_DEPTH_M1    127
0x0000e8:          000a 010d - cmd0.NPU_SET_IFM_IB_END           10
0x0000ec:          001e 012d - cmd0.NPU_SET_AB_START             30
0x0000f0:          0000 0124 - cmd0.NPU_SET_ACC_FORMAT            0
0x0000f4: 40000000 001e 4024 - cmd1.NPU_SET_OFM_SCALE            30
0x0000fc:          0000 012f - cmd0.NPU_SET_BLOCKDEP              0
0x000100:          0001 0005 - cmd0.NPU_OP_POOL                   1
0x000104: 000003f0 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x00010c: 00000010 0000 4006 - cmd1.NPU_SET_IFM_STRIDE_C          0
0x000114: 00000010 0000 4004 - cmd1.NPU_SET_IFM_STRIDE_X          0
0x00011c:          ff80 0109 - cmd0.NPU_SET_IFM_ZERO_POINT    65408
0x000120:          0041 0105 - cmd0.NPU_SET_IFM_PRECISION        65
0x000124: 00000000 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x00012c:          03ef 0113 - cmd0.NPU_SET_OFM_DEPTH_M1       1007
0x000130: 000003f0 0000 4015 - cmd1.NPU_SET_OFM_STRIDE_Y          0
0x000138:          fffd 0118 - cmd0.NPU_SET_OFM_ZERO_POINT    65533
0x00013c:          0041 0114 - cmd0.NPU_SET_OFM_PRECISION        65
0x000140:          0000 0128 - cmd0.NPU_SET_WEIGHT_REGION         0
0x000144: 00004ec0 0000 4020 - cmd1.NPU_SET_WEIGHT_BASE           0
0x00014c: 000d90f0 0000 4021 - cmd1.NPU_SET_WEIGHT_LENGTH         0
0x000154:          0000 0129 - cmd0.NPU_SET_SCALE_REGION          0
0x000158: 00002760 0000 4022 - cmd1.NPU_SET_SCALE_BASE            0
0x000160: 00002760 0000 4023 - cmd1.NPU_SET_SCALE_LENGTH          0
0x000168:          0003 012f - cmd0.NPU_SET_BLOCKDEP              3
0x00016c:          0000 0002 - cmd0.NPU_OP_CONV                   0
0x000170: 00000000 0000 4000 - cmd1.NPU_SET_IFM_BASE0             0
0x000178:          03ef 0104 - cmd0.NPU_SET_IFM_DEPTH_M1       1007
0x00017c: 000003f0 0000 4005 - cmd1.NPU_SET_IFM_STRIDE_Y          0
0x000184:          fffd 0109 - cmd0.NPU_SET_IFM_ZERO_POINT    65533
0x000188: 000003f0 0000 4010 - cmd1.NPU_SET_OFM_BASE0             0
0x000190: 00000001 0000 4016 - cmd1.NPU_SET_OFM_STRIDE_C          0
0x000198: 000003f0 0000 4014 - cmd1.NPU_SET_OFM_STRIDE_X          0
0x0001a0:          007d 0118 - cmd0.NPU_SET_OFM_ZERO_POINT      125
0x0001a4:          0100 0114 - cmd0.NPU_SET_OFM_PRECISION       256
0x0001a8:          0000 0126 - cmd0.NPU_SET_ACTIVATION_MIN        0
0x0001ac:          00ff 0127 - cmd0.NPU_SET_ACTIVATION_MAX      255
0x0001b0:          0000 012f - cmd0.NPU_SET_BLOCKDEP              0
0x0001b4:          0001 0005 - cmd0.NPU_OP_POOL                   1
0x0001b8:          ffff 0000 - cmd0.NPU_OP_STOP               65535
Number of commands = 82
Command stream length = 444 bytes

################################################################################
Tensor Allocation for mem_area OffChipFlash, of mem_type_set (Permanent_CPU), using allocator LinearAlloc, in Cpu subgraph:
Start Time -   End Time: Start Addr -   End Addr: Tensor Size: Memory Usage: Purpose     : Name
         0 -          3:        0x0 -      0x1e0:         480:       909712: FeatureMap  : main_split_1_command_stream
         0 -          3:      0x1e0 -    0xde190:      909232:       909712: FeatureMap  : main_split_1_flash
Allocation Peak Tensor Size:     909712 (   0xde190) Bytes   888.39 KiB
Allocation Peak Memory Usage:    909712 (   0xde190) Bytes   888.39 KiB
Allocation Overhead:                  0 Bytes (0.00 %)

Network summary for tflite_model
Accelerator configuration               Ethos_U55_256
System configuration             Ethos_U55_High_End_Embedded
Memory mode                               Shared_Sram
Accelerator clock                                 500 MHz
Design peak SRAM bandwidth                       3.73 GB/s
Design peak Off-chip Flash bandwidth             0.47 GB/s

Total SRAM used                                  1.97 KiB
Total Off-chip Flash used                      888.39 KiB

CPU operators = 0 (0.0%)
NPU operators = 3 (100.0%)

Average SRAM bandwidth                           0.01 GB/s
Input   SRAM bandwidth                           0.01 MB/batch
Weight  SRAM bandwidth                           0.00 MB/batch
Output  SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth                           0.01 MB/batch
Total   SRAM bandwidth            per input      0.01 MB/inference (batch size 1)

Average Off-chip Flash bandwidth                 0.48 GB/s
Input   Off-chip Flash bandwidth                 0.00 MB/batch
Weight  Off-chip Flash bandwidth                 0.86 MB/batch
Output  Off-chip Flash bandwidth                 0.00 MB/batch
Total   Off-chip Flash bandwidth                 0.87 MB/batch
Total   Off-chip Flash bandwidth  per input      0.87 MB/inference (batch size 1)

Original Weights Size                          771.75 KiB
NPU Encoded Weights Size                       868.23 KiB

Neural network macs                            792064 MACs/batch

Illustrate Vela Compiled Model in Netron? (y/n)
++ Converting tflite_model_vela.tflite to    tflite_model_vela.tflite.cc
